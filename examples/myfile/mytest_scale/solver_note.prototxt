# The train/test net protocol buffer definition
#也可用train_net和test_net来对训练模型和测试模型分别设定。例如：
###train_net: "examples/hdf5_classification/logreg_auto_train.prototxt"
###test_net: "examples/hdf5_classification/logreg_auto_test.prototxt"
net: "examples/mnist/lenet_train_test.prototxt"
#需要与test layer中的batch_size结合起来理解。两值相乘得数据总数。
#mnist数据中测试样本总数为10000，一次性执行全部数据效率很低，
#因此我们将测试数据分成几个批次来执行，每个批次的数量就是batch_size。
#假设我们设置batch_size为100，则需要迭代100次才能将10000个数据全部执行完。
#因此test_iter设置为100。执行完一次全部数据，称之为一个epoch
test_iter: 100
#500训练500次测试一次
test_interval: 500

#接下来的四个值用于学习率的设置。
# The base learning rate, momentum and the weight decay of the network.
#只要是梯度下降法来求解优化，都会有一个学习率，也叫步长。
#base_lr用于设置基础学习率，在迭代的过程中，可以对基础学习率进行调整。怎
#么样进行调整，就是调整的策略，由lr_policy来设置。
base_lr: 0.01 #设置基础学习率
#lr_policy可选值为以下：
#- fixed:　　 保持base_lr不变.
#- step: 　　 如果设置为step,则还需要设置一个stepsize,  返回 base_lr * gamma ^ (floor(iter / stepsize)),其中iter表示当前的迭代次数
#- exp:   　　返回base_lr * gamma ^ iter， iter为当前迭代次数
#- inv:　　    如果设置为inv,还需要设置一个power, 返回base_lr * (1 + gamma * iter) ^ (- power)
#- multistep: 如果设置为multistep,则还需要设置一个stepvalue。这个参数和step很相似，step是均匀等间隔变化，而multistep则是根据                                 stepvalue值变化
#- poly: 　　  学习率进行多项式误差, 返回 base_lr (1 - iter/max_iter) ^ (power)
#- sigmoid:　学习率进行sigmod衰减，返回 base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))
lr_policy: "inv"
gamma: 0.0001
power: 0.75


momentum: 0.9
type: SGD
weight_decay: 0.0005 #防止过拟合

display: 100
max_iter: 20000

snapshot: 5000 #默认为0 不保存
#还可以设置snapshot_diff，是否保存梯度值，默认为false,不保存。
#也可以设置snapshot_format，保存的类型。有两种选择：HDF5 和BINARYPROTO ，默认为BINARYPROTO
snapshot_prefix: "examples/mnist/lenet"
# solver mode: CPU or GPU
solver_mode: CPU